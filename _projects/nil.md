---
layout: page
title: NIL - No-data Imitation Learning
description:
img: assets/img/publication_preview/NIL.jpg
# importance: 1
# category: work
# related_publications: true
---

<style>
  .post-title {
    text-align: center;
    margin-top: -2rem;
  }
</style>

<div class="row mt-3">
    <div class="col-md-8 offset-md-2 text-center">
        <div class="authors mt-3">
            <span class="author-block"><a>Mert Albaba</a><sup>1,2</sup>,</span>
            <span class="author-block"><a>Chenhao Li</a><sup>1</sup>,</span>
            <span class="author-block"><a>Markos Diomataris</a><sup>1,2</sup>,</span>
            <span class="author-block"><a>Omid Taheri</a><sup>2</sup>,</span> <br/>
            <span class="author-block"><a>Andreas Krause</a><sup>1</sup>,</span>
            <span class="author-block"><a>Michael J. Black</a><sup>2</sup></span>
        </div>
        <div class="affiliations mt-2">
            <sup>1</sup>ETH Zürich &nbsp;&nbsp; <sup>2</sup>Max Planck Institute for Intelligent Systems
        </div>
        <div class="links mt-3">
            <a href="https://arxiv.org/abs/2503.10626" class="btn btn-dark" target="_blank" rel="noopener noreferrer">
                <i class="fas fa-file-pdf"></i> Paper
            </a>
            <a href="#video" class="btn btn-dark"> <i class="fas fa-video"></i> Video </a>
        </div>
    </div>
</div>
<hr>

<div class="row justify-content-center">
    <div class="col-md-10">
        <h3 class="text-center" style="margin-bottom: 1rem;">We teach robots to move by showing them AI-generated videos, with no need for real-world motion data.</h3>
        <div id="video">
            {% assign video_path = "assets/video/nil.mp4" | relative_url %}
            <video controls preload="metadata" class="img-fluid rounded z-depth-1">
                <source src="{{ video_path }}" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
    </div>
</div>

<section class="section">
    <div class="row">
        <div class="col-md-10 offset-md-1">
            <h2 class="title is-3 text-center">Abstract</h2>
            <div class="content">
                <p>
                Acquiring physically plausible motor skills across diverse and unconventional morphologies—including humanoid robots, quadrupeds, and animals—is essential for advancing character simulation and robotics. Traditional methods like reinforcement learning (RL) are task-specific and require extensive reward engineering. Imitation learning offers an alternative but relies on high-quality expert demonstrations, which are difficult to obtain for non-human morphologies. We propose <b>NIL (No-data Imitation Learning)</b>, a data-independent approach that learns 3D motor skills from 2D videos generated by pre-trained video diffusion models. We guide the learning process by calculating a reward based on the similarity between video embeddings from vision transformers and the Intersection over Union (IoU) of segmentation masks. We demonstrate that NIL outperforms baselines trained on 3D motion-capture data in humanoid locomotion tasks, effectively replacing data collection with data generation.
                </p>
            </div>
        </div>
    </div>
</section>

<hr>
<section class="section">
    <div class="row">
        <div class="col-md-10 offset-md-1">
            <h2 class="title is-3 text-center">Method</h2>
        </div>
    </div>
    <div class="row">
        <div class="col-md-10 offset-md-1">
            {% include figure.liquid loading="eager" path="assets/img/publication_preview/NIL_method.jpg" title="NIL Method" class="img-fluid rounded z-depth-1" %}
        </div>
    </div>
    <div class="row mt-3">
        <div class="col-md-10 offset-md-1">
            <p>
            <b>No-data Imitation Learning (NIL)</b> learns physically plausible 3D motor skills from 2D videos generated on-the-fly. The process has two stages:
            </p>
            <ul>
                <li><b>Stage 1: Reference Video Generation.</b> Given an initial frame of the robot and a text prompt (e.g., "Robot is walking"), a pre-trained video diffusion model generates a reference video of the desired skill.</li>
                <li><b>Stage 2: Policy Learning via Video Comparison.</b> A control policy is trained in a physical simulator. The reward function encourages the agent to mimic the generated video by measuring the similarity between the agent's rendered video and the reference video.</li>
            </ul>
            <p>This reward combines three key components:
                <ol>
                    <li><b>Video Encoding Similarity:</b> The L2 distance between video embeddings from a TimeSformer model.</li>
                    <li><b>Segmentation Mask Similarity:</b> The Intersection over Union (IoU) between the agent's segmentation masks in both videos.</li>
                    <li><b>Regularization:</b> Penalties on joint torques, velocities, and foot-sliding to ensure smooth, stable motion.</li>
                </ol>
            </p>
        </div>
    </div>
</section>

<hr>
<section class="section">
    <div class="row">
        <div class="col-md-10 offset-md-1">
            <h2 class="title is-3 text-center">Results</h2>
        </div>
    </div>
    <div class="row justify-content-center">
        <div class="col-md-10">
            <h3 class="text-center">Comparison to Baselines</h3>
            <p class="text-center">
            NIL successfully learns locomotion for various robots, including humanoids and quadrupeds. Crucially, NIL achieves this without any expert motion data, yet it outperforms state-of-the-art methods that are trained on 25 curated 3D motion-capture demonstrations. Below, we compare NIL (trained on generated video) to AMP (trained on 3D MoCap data).
            </p>
            <div class="row">
                <div class="col-md-10">
                    <h5 class="text-center">Unitree H1 Humanoid and Unitree A1 Quadruped</h5>
                    {% assign video_path = "assets/video/results1.mp4" | relative_url %}
                    <video controls preload="metadata" class="img-fluid rounded z-depth-1">
                        <source src="{{ video_path }}" type="video/mp4">
                    </video>
                </div>
                <div class="col-md-10 mt-4">
                    <h5 class="text-center">Unitree H1 Humanoid and Talos Humanoid</h5>
                    {% assign video_path2 = "assets/video/results2.mp4" | relative_url %}
                    <video controls preload="metadata" class="img-fluid rounded z-depth-1">
                        <source src="{{ video_path2 }}" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
    </div>
    <div class="row justify-content-center mt-5">
        <div class="col-md-10">
            <h3 class="text-center">Importance of Reward Components</h3>
            <p class="text-center">
            Our reward function combines video similarity, IoU similarity, and regularization. Removing any component degrades performance, leading to jittery, distorted, or suboptimal motion. Using all components together produces a stable and natural walking gait.
            </p>
            {% assign video_path3 = "assets/video/rewardfunctioncomps.mp4" | relative_url %}
            <video controls preload="metadata" class="img-fluid rounded z-depth-1">
                <source src="{{ video_path3 }}" type="video/mp4">
            </video>
        </div>
    </div>
    <div class="row justify-content-center mt-5">
        <div class="col-md-10">
            <h3 class="text-center">Impact of Video Diffusion Models</h3>
            <p class="text-center">
            The performance of NIL is directly linked to the quality of the underlying video diffusion model. As these models improve, NIL's ability to learn complex and natural behaviors also improves, demonstrating a promising path for future progress.
            </p>
            <div class="row">
                <div class="col-md-10">
                    <h5 class="text-center">Comparison of Different Models</h5>
                    {% assign video_path4 = "assets/video/videomodels.mp4" | relative_url %}
                    <video controls preload="metadata" class="img-fluid rounded z-depth-1">
                        <source src="{{ video_path4 }}" type="video/mp4">
                    </video>
                </div>
                <div class="col-md-10 mt-4">
                    <h5 class="text-center">Effect of Model Improvements</h5>
                    {% assign video_path5 = "assets/video/videomodelimprovements.mp4" | relative_url %}
                    <video controls preload="metadata" class="img-fluid rounded z-depth-1">
                        <source src="{{ video_path5 }}" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
    </div>
</section>
<section class="section" id="BibTeX">
    <div class="is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{albaba2025nilnodataimitationlearning,
      title={NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models}, 
      author={Mert Albaba and Chenhao Li and Markos Diomataris and Omid Taheri and Andreas Krause and Michael Black},
      year={2025},
      eprint={2503.10626},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.10626}, 
    }</code></pre>
    </div>
</section> 